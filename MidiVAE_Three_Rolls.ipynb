{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "255dfcc6",
   "metadata": {},
   "source": [
    "### This notebook is to build modified MIDI-VAE with pitch, duration and velocity rolls. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "712033ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from music21 import *\n",
    "import numpy as np\n",
    "from fractions import Fraction\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e8e2ad",
   "metadata": {},
   "source": [
    "## Declare three index dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1cfec03",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdict = {0.25: 0,\n",
    " 0.5: 1,\n",
    " 0.75: 2,\n",
    " 1.0: 3,\n",
    " 1.25: 4,\n",
    " 1.5: 5,\n",
    " 1.75: 6,\n",
    " 2.0: 7,\n",
    " 2.25: 8,\n",
    " 2.5: 9,\n",
    " 2.75: 10,\n",
    " 3.0: 11,\n",
    " 3.25: 12,\n",
    " 4.0: 13,\n",
    " 5.0: 14,\n",
    " 6.0: 15,\n",
    " 3.5: 16,\n",
    " 3.75: 17,\n",
    " 4.25: 18,\n",
    " 4.5: 19,\n",
    " 4.75: 20,\n",
    " 5.25: 21,\n",
    " 5.5: 22,\n",
    " 7.0: 23,\n",
    " Fraction(1, 6): 24,\n",
    " Fraction(5, 12): 25,\n",
    " Fraction(2, 3): 26,\n",
    " Fraction(5, 3): 27,\n",
    " Fraction(23, 12): 28,\n",
    " Fraction(8, 3): 29,\n",
    " Fraction(11, 3): 30,\n",
    " Fraction(1, 3): 31,\n",
    " Fraction(1, 12): 32,\n",
    " Fraction(5, 6): 33,\n",
    " Fraction(4, 3): 34,\n",
    " Fraction(7, 3): 35,\n",
    " Fraction(10, 3): 36}\n",
    "\n",
    "pdict = {24: 0,\n",
    " 25: 1,\n",
    " 26: 2,\n",
    " 27: 3,\n",
    " 28: 4,\n",
    " 29: 5,\n",
    " 30: 6,\n",
    " 31: 7,\n",
    " 32: 8,\n",
    " 33: 9,\n",
    " 34: 10,\n",
    " 35: 11,\n",
    " 36: 12,\n",
    " 37: 13,\n",
    " 38: 14,\n",
    " 39: 15,\n",
    " 40: 16,\n",
    " 41: 17,\n",
    " 42: 18,\n",
    " 43: 19,\n",
    " 44: 20,\n",
    " 45: 21,\n",
    " 46: 22,\n",
    " 47: 23,\n",
    " 48: 24,\n",
    " 49: 25,\n",
    " 50: 26,\n",
    " 51: 27,\n",
    " 52: 28,\n",
    " 53: 29,\n",
    " 54: 30,\n",
    " 55: 31,\n",
    " 56: 32,\n",
    " 57: 33,\n",
    " 58: 34,\n",
    " 59: 35,\n",
    " 60: 36,\n",
    " 61: 37,\n",
    " 62: 38,\n",
    " 63: 39,\n",
    " 64: 40,\n",
    " 65: 41,\n",
    " 66: 42,\n",
    " 67: 43,\n",
    " 68: 44,\n",
    " 69: 45,\n",
    " 70: 46,\n",
    " 71: 47,\n",
    " 72: 48,\n",
    " 73: 49,\n",
    " 74: 50,\n",
    " 75: 51,\n",
    " 76: 52,\n",
    " 77: 53,\n",
    " 78: 54,\n",
    " 79: 55,\n",
    " 80: 56,\n",
    " 81: 57,\n",
    " 82: 58,\n",
    " 83: 59,\n",
    " 84: 60,\n",
    " 85: 61,\n",
    " 86: 62,\n",
    " 87: 63,\n",
    " 88: 64,\n",
    " 89: 65,\n",
    " 90: 66,\n",
    " 91: 67,\n",
    " 92: 68,\n",
    " 93: 69,\n",
    " 94: 70,\n",
    " 95: 71,\n",
    " 96: 72,\n",
    " 97: 73,\n",
    " 98: 74,\n",
    " 99: 75,\n",
    " 100: 76,\n",
    " 101: 77}\n",
    "\n",
    "vdict = {20: 0,\n",
    " 21: 1,\n",
    " 22: 2,\n",
    " 23: 3,\n",
    " 24: 4,\n",
    " 25: 5,\n",
    " 26: 6,\n",
    " 27: 7,\n",
    " 28: 8,\n",
    " 29: 9,\n",
    " 30: 10,\n",
    " 31: 11,\n",
    " 32: 12,\n",
    " 33: 13,\n",
    " 34: 14,\n",
    " 35: 15,\n",
    " 36: 16,\n",
    " 37: 17,\n",
    " 38: 18,\n",
    " 39: 19,\n",
    " 40: 20,\n",
    " 41: 21,\n",
    " 42: 22,\n",
    " 43: 23,\n",
    " 44: 24,\n",
    " 45: 25,\n",
    " 46: 26,\n",
    " 47: 27,\n",
    " 48: 28,\n",
    " 49: 29,\n",
    " 50: 30,\n",
    " 51: 31,\n",
    " 52: 32,\n",
    " 53: 33,\n",
    " 54: 34,\n",
    " 55: 35,\n",
    " 56: 36,\n",
    " 57: 37,\n",
    " 58: 38,\n",
    " 59: 39,\n",
    " 60: 40,\n",
    " 61: 41,\n",
    " 62: 42,\n",
    " 63: 43,\n",
    " 64: 44,\n",
    " 65: 45,\n",
    " 66: 46,\n",
    " 67: 47,\n",
    " 68: 48,\n",
    " 69: 49,\n",
    " 70: 50,\n",
    " 71: 51,\n",
    " 72: 52,\n",
    " 73: 53,\n",
    " 74: 54,\n",
    " 75: 55,\n",
    " 76: 56,\n",
    " 77: 57,\n",
    " 78: 58,\n",
    " 79: 59,\n",
    " 80: 60,\n",
    " 81: 61,\n",
    " 82: 62,\n",
    " 83: 63,\n",
    " 84: 64,\n",
    " 85: 65,\n",
    " 86: 66,\n",
    " 87: 67,\n",
    " 88: 68,\n",
    " 89: 69,\n",
    " 90: 70,\n",
    " 91: 71,\n",
    " 92: 72,\n",
    " 93: 73,\n",
    " 94: 74,\n",
    " 95: 75}\n",
    "\n",
    "id2tdict = {i:t for t,i in tdict.items()}\n",
    "id2pdict = {i:p for p,i in pdict.items()}\n",
    "id2vdict = {i:v for v,i in vdict.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d13a3a5",
   "metadata": {},
   "source": [
    "## Generate the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ee8b7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, right, left, Y, transform = None):\n",
    "        self.right = right\n",
    "        self.left = left\n",
    "        self.Y = Y\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.Y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.right[idx], self.left[idx], self.Y[idx]\n",
    "    \n",
    "\n",
    "def generate_data_loader(right_data, left_data, labels, batch_size):\n",
    "    \n",
    "    X = np.arange(right_data.shape[0])\n",
    "    y = labels\n",
    "    X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, stratify=y, random_state=23)\n",
    "    \n",
    "    # Split train into train-val\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_trainval, y_trainval, test_size=0.2, stratify=y_trainval, random_state=42)\n",
    "\n",
    "    right_train = right_data[X_train]\n",
    "    right_val = right_data[X_val]\n",
    "    right_test = right_data[X_test]\n",
    "    \n",
    "    left_train = left_data[X_train]\n",
    "    left_val = left_data[X_val]\n",
    "    left_test = left_data[X_test]    \n",
    "    \n",
    "    train_data = MusicDataset(torch.from_numpy(right_train).float(), \n",
    "                              torch.from_numpy(left_train).float(), \n",
    "                             torch.from_numpy(y_train).float())\n",
    "    val_data = MusicDataset(torch.from_numpy(right_val).float(), \n",
    "                            torch.from_numpy(left_val).float(),\n",
    "                           torch.from_numpy(y_val).float())\n",
    "    test_data = MusicDataset(torch.from_numpy(right_test).float(),\n",
    "                             torch.from_numpy(left_test).float(),\n",
    "                            torch.from_numpy(y_test).float())\n",
    "        \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_data, batch_size=batch_size, shuffle=False)\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_data, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_data, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1921273b",
   "metadata": {},
   "source": [
    "## Build MIDI-VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86e68d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MidiVAE(nn.Module):\n",
    "    def __init__(self, phid, thid, vhid, enc_hidden, z_dim):\n",
    "        super(MidiVAE, self).__init__()\n",
    "        \n",
    "        self.z_dim = z_dim\n",
    "        \n",
    "        self.lstm_enc_pitch = torch.nn.LSTM(\n",
    "                batch_first = True,\n",
    "                input_size = len(pdict),\n",
    "                hidden_size = phid,\n",
    "                num_layers = 1,\n",
    "                bidirectional = False)\n",
    "        \n",
    "        self.lstm_enc_time = torch.nn.LSTM(\n",
    "                batch_first = True,\n",
    "                input_size = len(tdict),\n",
    "                hidden_size = thid,\n",
    "                num_layers = 1,\n",
    "                bidirectional = False)\n",
    "        \n",
    "        \n",
    "        self.lstm_enc_velo = torch.nn.LSTM(\n",
    "                batch_first = True,\n",
    "                input_size = len(vdict),\n",
    "                hidden_size = vhid,\n",
    "                num_layers = 1,\n",
    "                bidirectional = False)\n",
    "        \n",
    "        \n",
    "        self.fc_enc1 = nn.Linear((phid+thid+vhid)*note_num, enc_hidden*note_num)\n",
    "        self.fc_enc2 = nn.Linear(enc_hidden*note_num, enc_hidden*note_num)\n",
    "        self.fc_enc3 = nn.Linear(enc_hidden*note_num, enc_hidden*note_num)\n",
    "        self.fc_enc4 = nn.Linear(enc_hidden*note_num, enc_hidden*note_num)\n",
    "        self.dropout_enc = nn.Dropout(p=0.5)\n",
    "        \n",
    "        self.fc_mu = nn.Linear(enc_hidden*(note_num//2), 3*self.z_dim)\n",
    "        self.fc_var = nn.Linear(enc_hidden*(note_num//2), 3*self.z_dim)\n",
    "        \n",
    "        self.fc_dec1 = nn.Linear(self.z_dim, 3*self.z_dim)\n",
    "        self.fc_dec2 = nn.Linear(3*self.z_dim, 6*self.z_dim)\n",
    "        self.fc_dec3 = nn.Linear(6*self.z_dim, 6*self.z_dim)\n",
    "        self.fc_dec4 = nn.Linear(6*self.z_dim, 9*self.z_dim)\n",
    "        \n",
    "        self.fc_p = nn.Linear(3*self.z_dim, phid*note_num)\n",
    "        self.fc_t = nn.Linear(3*self.z_dim, thid*note_num)\n",
    "        self.fc_v = nn.Linear(3*self.z_dim, vhid*note_num)\n",
    "        self.dropout_dec = nn.Dropout(p=0.3)\n",
    "        \n",
    "        self.lstm_dec_pitch = torch.nn.LSTM(\n",
    "                batch_first = True,\n",
    "                input_size = phid,\n",
    "                hidden_size = len(pdict),\n",
    "                num_layers = 1,\n",
    "                bidirectional = False)\n",
    "        \n",
    "        self.lstm_dec_time = torch.nn.LSTM(\n",
    "                batch_first = True,\n",
    "                input_size = thid,\n",
    "                hidden_size = len(tdict),\n",
    "                num_layers = 1,\n",
    "                bidirectional = False)\n",
    "        \n",
    "        self.lstm_dec_velo = torch.nn.LSTM(\n",
    "                batch_first = True,\n",
    "                input_size = vhid,\n",
    "                hidden_size = len(vdict),\n",
    "                num_layers = 1,\n",
    "                bidirectional = False)        \n",
    "        \n",
    "    \n",
    "    def encoder(self, data):\n",
    "        pitch = data[:,:,:len(pdict)]\n",
    "        time = data[:,:,len(pdict):len(pdict)+len(tdict)]\n",
    "        velo = data[:,:,len(pdict)+len(tdict):]\n",
    "        \n",
    "        x1,_ = self.lstm_enc_pitch(pitch)\n",
    "        x2,_ = self.lstm_enc_time(time)\n",
    "        x3,_ = self.lstm_enc_velo(velo)\n",
    "        del _\n",
    "        \n",
    "        # concat\n",
    "        x = torch.cat((x1, x2, x3), dim=2)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = F.relu(self.fc_enc1(x))\n",
    "        x = F.relu(self.fc_enc2(x))\n",
    "        x = F.relu(self.fc_enc3(x))\n",
    "        x = F.relu(self.fc_enc4(x))\n",
    "        x = self.dropout_enc(x)\n",
    "        x1, x2 = torch.split(x, [x.shape[1]//2, x.shape[1]//2], dim=1)\n",
    "        \n",
    "        return self.fc_mu(x1), self.fc_var(x2)\n",
    "    \n",
    "    \n",
    "    def reparameterize(self, mu, logvar, labels):\n",
    "        \n",
    "        msplit = torch.split(mu, [self.z_dim, self.z_dim, self.z_dim], dim=1)\n",
    "        vsplit = torch.split(logvar, [self.z_dim, self.z_dim, self.z_dim], dim=1)\n",
    "        new_z = torch.zeros(mu.shape[0], self.z_dim).to(device)\n",
    "        new_mu = torch.zeros(mu.shape[0], self.z_dim).to(device)\n",
    "        new_var = torch.zeros(logvar.shape[0], self.z_dim).to(device)\n",
    "        \n",
    "        # one hot \n",
    "        for i in range(3):\n",
    "            mi = msplit[i]\n",
    "            vi = vsplit[i]\n",
    "            new_mu += mi * labels[:,i].reshape(-1,1)\n",
    "            new_var += vi * labels[:,i].reshape(-1,1)\n",
    "            stdev = torch.exp(0.5*vi)\n",
    "            eps = torch.randn_like(stdev)\n",
    "            z = mi + eps*stdev\n",
    "            new_z += z * labels[:,i].reshape(-1,1)\n",
    "\n",
    "        return new_z, new_mu, new_var\n",
    "    \n",
    "    \n",
    "    def decoder(self, z):\n",
    "        x = F.relu(self.fc_dec1(z))\n",
    "        x = F.relu(self.fc_dec2(x))\n",
    "        x = F.relu(self.fc_dec3(x))\n",
    "        x = F.relu(self.fc_dec4(x))\n",
    "        x = self.dropout_dec(x)\n",
    "        \n",
    "        x1, x2, x3 = torch.split(x, [x.shape[1]//3, x.shape[1]//3, x.shape[1]//3], dim=1)\n",
    "        x1 = F.relu(self.fc_p(x1))\n",
    "        x2 = F.relu(self.fc_t(x2))\n",
    "        x3 = F.relu(self.fc_v(x3))\n",
    "        \n",
    "        x1 = x1.reshape(x1.shape[0],note_num,-1)\n",
    "        x2 = x2.reshape(x2.shape[0],note_num,-1)\n",
    "        x3 = x3.reshape(x3.shape[0],note_num,-1)\n",
    "        \n",
    "        pitch,_ = self.lstm_dec_pitch(x1)\n",
    "        time,_ = self.lstm_dec_time(x2)\n",
    "        velo,_ = self.lstm_dec_velo(x3)\n",
    "        del _\n",
    "\n",
    "        new_p = torch.softmax(pitch, dim=2)\n",
    "        new_v = torch.softmax(velo, dim=2)\n",
    "        new_t = torch.softmax(time, dim=2)\n",
    "        recon_data = torch.cat((new_p, new_t, new_v), dim=2).to(device)        \n",
    "        \n",
    "        return recon_data\n",
    "    \n",
    "    \n",
    "    def forward(self, data, labels):\n",
    "        mu, logvar = self.encoder(data)\n",
    "        z, new_mu, new_var = self.reparameterize(mu, logvar, labels)\n",
    "        recon_data = self.decoder(z)\n",
    "        return recon_data, new_mu, new_var\n",
    "    \n",
    "    \n",
    "    def loss_func(self, recon_data, data, mu, logvar):\n",
    "        CE = F.cross_entropy(recon_data, data) # BCE = -Negative Log-likelihood\n",
    "        KLD = -1 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) # KL Divergence b/w q_\\phi(z|x) || p(z)\n",
    "        return CE + KLD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933e5290",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd02fd77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MidiVAE(\n",
      "  (lstm_enc_pitch): LSTM(78, 30, batch_first=True)\n",
      "  (lstm_enc_time): LSTM(12, 30, batch_first=True)\n",
      "  (lstm_enc_velo): LSTM(76, 30, batch_first=True)\n",
      "  (fc_enc1): Linear(in_features=4500, out_features=2500, bias=True)\n",
      "  (fc_enc2): Linear(in_features=2500, out_features=2500, bias=True)\n",
      "  (fc_enc3): Linear(in_features=2500, out_features=2500, bias=True)\n",
      "  (fc_enc4): Linear(in_features=2500, out_features=2500, bias=True)\n",
      "  (dropout_enc): Dropout(p=0.5, inplace=False)\n",
      "  (fc_mu): Linear(in_features=1250, out_features=768, bias=True)\n",
      "  (fc_var): Linear(in_features=1250, out_features=768, bias=True)\n",
      "  (fc_dec1): Linear(in_features=256, out_features=768, bias=True)\n",
      "  (fc_dec2): Linear(in_features=768, out_features=1536, bias=True)\n",
      "  (fc_dec3): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "  (fc_dec4): Linear(in_features=1536, out_features=2304, bias=True)\n",
      "  (fc_p): Linear(in_features=768, out_features=1500, bias=True)\n",
      "  (fc_t): Linear(in_features=768, out_features=1500, bias=True)\n",
      "  (fc_v): Linear(in_features=768, out_features=1500, bias=True)\n",
      "  (dropout_dec): Dropout(p=0.3, inplace=False)\n",
      "  (lstm_dec_pitch): LSTM(30, 78, batch_first=True)\n",
      "  (lstm_dec_time): LSTM(30, 12, batch_first=True)\n",
      "  (lstm_dec_velo): LSTM(30, 76, batch_first=True)\n",
      ")\n",
      "\n",
      " EPOCH 1/10 \t train loss 3.9667021800310183\n",
      "\n",
      " EPOCH 2/10 \t train loss 3.945529552606436\n",
      "\n",
      " EPOCH 3/10 \t train loss 3.945220404540711\n",
      "\n",
      " EPOCH 4/10 \t train loss 3.945278799431956\n",
      "\n",
      " EPOCH 5/10 \t train loss 3.945250357657756\n",
      "\n",
      " EPOCH 6/10 \t train loss 3.945240479249221\n",
      "\n",
      " EPOCH 7/10 \t train loss 3.9452524083292384\n",
      "\n",
      " EPOCH 8/10 \t train loss 3.9452538116705043\n",
      "\n",
      " EPOCH 9/10 \t train loss 3.9452556171308557\n",
      "\n",
      " EPOCH 10/10 \t train loss 3.9452572419093204\n"
     ]
    }
   ],
   "source": [
    "right_data = np.load('right_data_small.npy')\n",
    "left_data = np.load('left_data_small.npy')\n",
    "labels = np.load('labels_small.npy')\n",
    "batch_size = 5\n",
    "note_num = 50\n",
    "train_loader, val_loader, test_loader = generate_data_loader(right_data, left_data, labels, batch_size)\n",
    "del right_data\n",
    "del left_data\n",
    "del labels\n",
    "\n",
    "\n",
    "lr = 0.001\n",
    "phid = 30\n",
    "thid = 30\n",
    "vhid = 30\n",
    "enc_hidden = 50\n",
    "z_dim = 256\n",
    "num_epochs = 10\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu'\n",
    "\n",
    "\n",
    "vae = MidiVAE(phid, thid, vhid, enc_hidden, z_dim).to(device)\n",
    "print(vae)\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_epoch_loss = 0\n",
    "    vae.train()\n",
    "    for right, left, label in train_loader:\n",
    "        \n",
    "        data = right.to(device)\n",
    "        label = label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        recon_data, mu, logvar = vae(data, label)\n",
    "\n",
    "        train_loss = vae.loss_func(recon_data, data, mu, logvar)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        train_epoch_loss += train_loss.item()\n",
    "    \n",
    "    print('\\n EPOCH {}/{} \\t train loss {}'\\\n",
    "                  .format(epoch, num_epochs,train_epoch_loss/len(train_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9af9be",
   "metadata": {},
   "source": [
    "## Check the distribution N(0,1) and N(1,0.5) for latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e16e238f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([32, 46,  1, 39, 32, 44, 39,  1, 44, 29,  0,  1, 51,  0, 36,  3,  3, 39,\n",
      "        46, 44, 44, 50, 48,  0,  1, 49, 49,  1,  1,  1, 35, 47, 31, 16, 41, 44,\n",
      "         1,  1,  1,  1, 45,  1, 55, 55,  0, 12,  0, 39,  1,  0]) tensor([ 1,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  1,  0, 10,  1,  0,  0,  0,\n",
      "         1,  0,  0,  0,  0,  9,  0,  0,  0,  0,  1,  0,  7, 10,  2,  2,  0,  0,\n",
      "        10,  1,  0,  1,  3,  6,  1,  6, 10,  1, 10,  0,  1,  3]) tensor([ 2,  0,  8,  9,  6,  0,  3,  9,  6,  6,  6,  0,  6,  6,  6, 11, 11,  6,\n",
      "         0,  9,  0,  9,  6,  6,  6,  0,  6,  6, 11,  6,  6,  0,  4,  4,  6,  6,\n",
      "         6,  4,  4,  6,  6,  6,  6,  9,  6,  6,  6,  9,  6,  6])\n"
     ]
    }
   ],
   "source": [
    "# N(0,1)\n",
    "\n",
    "vi = torch.zeros(1,256)\n",
    "mi = torch.zeros(1,256)\n",
    "stdev = torch.exp(0.5*vi)\n",
    "eps = torch.randn_like(stdev)\n",
    "z = mi + eps*stdev\n",
    "out = vae.decoder(z.to(device))\n",
    "right_out = out.squeeze()\n",
    "right_p = right_out[:,:len(pdict)]\n",
    "right_v = right_out[:,len(pdict):len(pdict)+len(vdict)]\n",
    "right_t = right_out[:,len(pdict)+len(vdict):]\n",
    "\n",
    "rp = torch.argmax(right_p, dim=1).cpu().detach()\n",
    "rt = torch.argmax(right_t, dim=1).cpu().detach()\n",
    "rv = torch.argmax(right_v, dim=1).cpu().detach()\n",
    "\n",
    "print(rp, rt, rv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0ced3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([32, 46, 39, 39, 32, 44, 39,  1, 44, 29,  0,  1, 51,  0, 36,  3,  3, 39,\n",
      "        46, 44, 44, 50, 48, 48,  1, 49, 49,  1,  1,  1, 35, 47, 31, 16, 41, 44,\n",
      "         1,  1,  1,  1, 45, 46, 55, 55,  0, 12,  0, 39,  1,  0]) tensor([ 1,  0,  0,  0,  0,  9,  0,  1,  0,  0,  0,  1,  0, 10,  1,  0,  0,  0,\n",
      "         1,  0,  0,  0,  0,  9,  0,  0,  0,  0,  1,  0,  7, 10,  2,  2,  0,  0,\n",
      "        10,  1,  0,  1,  3,  6,  1,  6, 10,  1, 10,  0,  1,  3]) tensor([ 2,  0,  8,  9,  6,  0,  3,  9,  6,  6,  6,  9,  6,  6,  6, 11, 11,  6,\n",
      "         0,  9,  0,  9,  6,  6,  6,  0,  6,  6, 11,  6,  6,  9,  4,  4,  6,  6,\n",
      "         6,  4,  4,  6,  6,  6,  6,  9,  6,  6,  6,  9,  6,  6])\n"
     ]
    }
   ],
   "source": [
    "# N~(1,0.5)\n",
    "vi = torch.ones(1,256)\n",
    "mi = torch.ones(1,256)\n",
    "stdev = torch.exp(-0.7*vi)\n",
    "eps = torch.randn_like(stdev)\n",
    "z = mi + eps*stdev\n",
    "out = vae.decoder(z.to(device))\n",
    "\n",
    "right_out = out.squeeze()\n",
    "right_p = right_out[:,:len(pdict)]\n",
    "right_v = right_out[:,len(pdict):len(pdict)+len(vdict)]\n",
    "right_t = right_out[:,len(pdict)+len(vdict):]\n",
    "\n",
    "rp = torch.argmax(right_p, dim=1).cpu().detach()\n",
    "rt = torch.argmax(right_t, dim=1).cpu().detach()\n",
    "rv = torch.argmax(right_v, dim=1).cpu().detach()\n",
    "\n",
    "print(rp, rt, rv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01ce75e",
   "metadata": {},
   "source": [
    "## Generate new music (Change the probabilities here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd009edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "right_data = np.load('right_data_small.npy')\n",
    "left_data = np.load('left_data_small.npy')\n",
    "labels = np.load('labels_small.npy')\n",
    "\n",
    "####### Change this variable to alter the mixed percentage for three composers #####\n",
    "prob = torch.tensor([0.6, 0.2, 0.2]).to(device)\n",
    "\n",
    "for i in range(3):\n",
    "    idx = random.choices(np.where(labels[:,i] == 1)[0])[0]\n",
    "    data = np.expand_dims(right_data[idx], axis=0)\n",
    "    data = torch.from_numpy(data).float()\n",
    "    data = data.to(device)\n",
    "    \n",
    "    vae.eval()\n",
    "    mu, logvar = vae.encoder(data)\n",
    "    mi = torch.split(mu, [z_dim, z_dim, z_dim], dim=1)[i]\n",
    "    vi = torch.split(logvar, [z_dim, z_dim, z_dim], dim=1)[i]\n",
    "    new_z = torch.zeros(mu.shape[0], z_dim).to(device)\n",
    "    stdev = torch.exp(0.5*vi)\n",
    "    eps = torch.randn_like(stdev)\n",
    "    z = mi + eps*stdev\n",
    "    new_z += z * prob[i].reshape(-1,1)\n",
    "\n",
    "output = vae.decoder(new_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20bdbe72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([73, 46, 39, 39, 32, 44, 39,  1, 46, 50, 20, 58, 51, 16, 36, 43, 60, 48,\n",
      "        46, 46, 61, 60, 48, 48,  1, 49, 55,  1,  1,  1, 35, 47, 38, 36, 77, 44,\n",
      "         1, 13,  1,  1, 45, 46, 55, 55,  1, 12, 21, 39,  1,  0]) tensor([ 1,  3,  0,  0,  3,  9,  9,  3,  8,  8,  3,  3,  0, 10,  7,  0,  5,  3,\n",
      "         3,  0,  0,  0,  0,  9,  0,  1,  7,  1,  1,  0,  7, 10,  2,  2,  6,  0,\n",
      "        10, 10,  0,  9,  3,  6,  6,  6, 10,  1, 10,  7,  5,  3]) tensor([ 2,  9,  8,  9,  6, 11,  6,  9,  6,  6,  9,  9,  6,  6,  6, 11, 11,  6,\n",
      "         1,  9,  6,  9,  6,  6,  9,  6,  6,  9, 11,  6,  6,  9,  4,  4,  6,  6,\n",
      "         6, 10,  4,  6,  6,  6,  6,  9,  7,  6,  6,  9,  6,  6])\n"
     ]
    }
   ],
   "source": [
    "right_out = output.squeeze()\n",
    "\n",
    "right_p = right_out[:,:len(pdict)]\n",
    "right_v = right_out[:,len(pdict):len(pdict)+len(vdict)]\n",
    "right_t = right_out[:,len(pdict)+len(vdict):]\n",
    "\n",
    "rp = torch.argmax(right_p, dim=1).cpu().detach()\n",
    "rt = torch.argmax(right_t, dim=1).cpu().detach()\n",
    "rv = torch.argmax(right_v, dim=1).cpu().detach()\n",
    "\n",
    "print(rp, rt, rv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfaf235f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n",
      "2/3\n",
      "1/6\n",
      "2/3\n",
      "1.75\n",
      "1/12\n",
      "1.75\n",
      "2/3\n",
      "1.75\n",
      "1.75\n",
      "2/3\n",
      "2/3\n",
      "1.75\n",
      "1.75\n",
      "1.75\n",
      "1/12\n",
      "1/12\n",
      "1.75\n",
      "0.5\n",
      "2/3\n",
      "1.75\n",
      "2/3\n",
      "1.75\n",
      "1.75\n",
      "2/3\n",
      "1.75\n",
      "1.75\n",
      "2/3\n",
      "1/12\n",
      "1.75\n",
      "1.75\n",
      "2/3\n",
      "1.25\n",
      "1.25\n",
      "1.75\n",
      "1.75\n",
      "1.75\n",
      "1/3\n",
      "1.25\n",
      "1.75\n",
      "1.75\n",
      "1.75\n",
      "1.75\n",
      "2/3\n",
      "2\n",
      "1.75\n",
      "1.75\n",
      "2/3\n",
      "1.75\n",
      "1.75\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'music_3rolls.mid'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert2notes(pid, tid, vid, offset,right=True):\n",
    "\n",
    "    result = note.Note(id2pdict[pid])\n",
    "    result.offset = offset\n",
    "    result.storedInstrument = instrument.Piano()\n",
    "    result.duration.quarterLength = id2tdict[tid]\n",
    "    print(id2tdict[tid])\n",
    "    result.volume.velocity = id2vdict[vid]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "notes = []\n",
    "offset = 0\n",
    "for pid, vid, tid in zip(rp, rv, rt):\n",
    "\n",
    "    n = convert2notes(pid.item(), vid.item(), tid.item(), offset)\n",
    "    offset += n.duration.quarterLength\n",
    "    notes.append(n)\n",
    "\n",
    "midi_stream = stream.Stream(notes)\n",
    "midi_stream.write('midi', fp='music_3rolls.mid')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
