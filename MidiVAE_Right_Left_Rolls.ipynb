{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69ed01c7",
   "metadata": {},
   "source": [
    "### This notebook is to build modified MIDI-VAE with right and left rolls. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29d996cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from music21 import *\n",
    "import numpy as np\n",
    "from fractions import Fraction\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49215a97",
   "metadata": {},
   "source": [
    "## Declare three index dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbf07c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdict = {0.25: 0,\n",
    " 0.5: 1,\n",
    " 0.75: 2,\n",
    " 1.0: 3,\n",
    " 1.25: 4,\n",
    " 1.5: 5,\n",
    " 1.75: 6,\n",
    " 2.0: 7,\n",
    " 2.25: 8,\n",
    " 2.5: 9,\n",
    " 2.75: 10,\n",
    " 3.0: 11,\n",
    " 3.25: 12,\n",
    " 4.0: 13,\n",
    " 5.0: 14,\n",
    " 6.0: 15,\n",
    " 3.5: 16,\n",
    " 3.75: 17,\n",
    " 4.25: 18,\n",
    " 4.5: 19,\n",
    " 4.75: 20,\n",
    " 5.25: 21,\n",
    " 5.5: 22,\n",
    " 7.0: 23,\n",
    " Fraction(1, 6): 24,\n",
    " Fraction(5, 12): 25,\n",
    " Fraction(2, 3): 26,\n",
    " Fraction(5, 3): 27,\n",
    " Fraction(23, 12): 28,\n",
    " Fraction(8, 3): 29,\n",
    " Fraction(11, 3): 30,\n",
    " Fraction(1, 3): 31,\n",
    " Fraction(1, 12): 32,\n",
    " Fraction(5, 6): 33,\n",
    " Fraction(4, 3): 34,\n",
    " Fraction(7, 3): 35,\n",
    " Fraction(10, 3): 36}\n",
    "\n",
    "pdict = {24: 0,\n",
    " 25: 1,\n",
    " 26: 2,\n",
    " 27: 3,\n",
    " 28: 4,\n",
    " 29: 5,\n",
    " 30: 6,\n",
    " 31: 7,\n",
    " 32: 8,\n",
    " 33: 9,\n",
    " 34: 10,\n",
    " 35: 11,\n",
    " 36: 12,\n",
    " 37: 13,\n",
    " 38: 14,\n",
    " 39: 15,\n",
    " 40: 16,\n",
    " 41: 17,\n",
    " 42: 18,\n",
    " 43: 19,\n",
    " 44: 20,\n",
    " 45: 21,\n",
    " 46: 22,\n",
    " 47: 23,\n",
    " 48: 24,\n",
    " 49: 25,\n",
    " 50: 26,\n",
    " 51: 27,\n",
    " 52: 28,\n",
    " 53: 29,\n",
    " 54: 30,\n",
    " 55: 31,\n",
    " 56: 32,\n",
    " 57: 33,\n",
    " 58: 34,\n",
    " 59: 35,\n",
    " 60: 36,\n",
    " 61: 37,\n",
    " 62: 38,\n",
    " 63: 39,\n",
    " 64: 40,\n",
    " 65: 41,\n",
    " 66: 42,\n",
    " 67: 43,\n",
    " 68: 44,\n",
    " 69: 45,\n",
    " 70: 46,\n",
    " 71: 47,\n",
    " 72: 48,\n",
    " 73: 49,\n",
    " 74: 50,\n",
    " 75: 51,\n",
    " 76: 52,\n",
    " 77: 53,\n",
    " 78: 54,\n",
    " 79: 55,\n",
    " 80: 56,\n",
    " 81: 57,\n",
    " 82: 58,\n",
    " 83: 59,\n",
    " 84: 60,\n",
    " 85: 61,\n",
    " 86: 62,\n",
    " 87: 63,\n",
    " 88: 64,\n",
    " 89: 65,\n",
    " 90: 66,\n",
    " 91: 67,\n",
    " 92: 68,\n",
    " 93: 69,\n",
    " 94: 70,\n",
    " 95: 71,\n",
    " 96: 72,\n",
    " 97: 73,\n",
    " 98: 74,\n",
    " 99: 75,\n",
    " 100: 76,\n",
    " 101: 77}\n",
    "\n",
    "vdict = {20: 0,\n",
    " 21: 1,\n",
    " 22: 2,\n",
    " 23: 3,\n",
    " 24: 4,\n",
    " 25: 5,\n",
    " 26: 6,\n",
    " 27: 7,\n",
    " 28: 8,\n",
    " 29: 9,\n",
    " 30: 10,\n",
    " 31: 11,\n",
    " 32: 12,\n",
    " 33: 13,\n",
    " 34: 14,\n",
    " 35: 15,\n",
    " 36: 16,\n",
    " 37: 17,\n",
    " 38: 18,\n",
    " 39: 19,\n",
    " 40: 20,\n",
    " 41: 21,\n",
    " 42: 22,\n",
    " 43: 23,\n",
    " 44: 24,\n",
    " 45: 25,\n",
    " 46: 26,\n",
    " 47: 27,\n",
    " 48: 28,\n",
    " 49: 29,\n",
    " 50: 30,\n",
    " 51: 31,\n",
    " 52: 32,\n",
    " 53: 33,\n",
    " 54: 34,\n",
    " 55: 35,\n",
    " 56: 36,\n",
    " 57: 37,\n",
    " 58: 38,\n",
    " 59: 39,\n",
    " 60: 40,\n",
    " 61: 41,\n",
    " 62: 42,\n",
    " 63: 43,\n",
    " 64: 44,\n",
    " 65: 45,\n",
    " 66: 46,\n",
    " 67: 47,\n",
    " 68: 48,\n",
    " 69: 49,\n",
    " 70: 50,\n",
    " 71: 51,\n",
    " 72: 52,\n",
    " 73: 53,\n",
    " 74: 54,\n",
    " 75: 55,\n",
    " 76: 56,\n",
    " 77: 57,\n",
    " 78: 58,\n",
    " 79: 59,\n",
    " 80: 60,\n",
    " 81: 61,\n",
    " 82: 62,\n",
    " 83: 63,\n",
    " 84: 64,\n",
    " 85: 65,\n",
    " 86: 66,\n",
    " 87: 67,\n",
    " 88: 68,\n",
    " 89: 69,\n",
    " 90: 70,\n",
    " 91: 71,\n",
    " 92: 72,\n",
    " 93: 73,\n",
    " 94: 74,\n",
    " 95: 75}\n",
    "\n",
    "id2tdict = {i:t for t,i in tdict.items()}\n",
    "id2pdict = {i:p for p,i in pdict.items()}\n",
    "id2vdict = {i:v for v,i in vdict.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3273158f",
   "metadata": {},
   "source": [
    "## Generate the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a72929ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, right, left, Y, transform = None):\n",
    "        self.right = right\n",
    "        self.left = left\n",
    "        self.Y = Y\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.Y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.right[idx], self.left[idx], self.Y[idx]\n",
    "    \n",
    "\n",
    "def generate_data_loader(right_data, left_data, labels, batch_size):\n",
    "    \n",
    "    X = np.arange(right_data.shape[0])\n",
    "    y = labels\n",
    "    X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, stratify=y, random_state=23)\n",
    "    \n",
    "    # Split train into train-val\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_trainval, y_trainval, test_size=0.2, stratify=y_trainval, random_state=42)\n",
    "\n",
    "    right_train = right_data[X_train]\n",
    "    right_val = right_data[X_val]\n",
    "    right_test = right_data[X_test]\n",
    "    \n",
    "    left_train = left_data[X_train]\n",
    "    left_val = left_data[X_val]\n",
    "    left_test = left_data[X_test]    \n",
    "    \n",
    "    train_data = MusicDataset(torch.from_numpy(right_train).float(), \n",
    "                              torch.from_numpy(left_train).float(), \n",
    "                             torch.from_numpy(y_train).float())\n",
    "    val_data = MusicDataset(torch.from_numpy(right_val).float(), \n",
    "                            torch.from_numpy(left_val).float(),\n",
    "                           torch.from_numpy(y_val).float())\n",
    "    test_data = MusicDataset(torch.from_numpy(right_test).float(),\n",
    "                             torch.from_numpy(left_test).float(),\n",
    "                            torch.from_numpy(y_test).float())\n",
    "        \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_data, batch_size=batch_size, shuffle=False)\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_data, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_data, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d428758",
   "metadata": {},
   "source": [
    "## Build MIDI-VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3956fb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MidiVAE(nn.Module):\n",
    "    def __init__(self, input_size, lstm_hidden, enc_hidden, z_dim):\n",
    "        super(MidiVAE, self).__init__()\n",
    "        \n",
    "        self.z_dim = z_dim\n",
    "        \n",
    "        self.lstm_enc_right = torch.nn.LSTM(\n",
    "                batch_first = True,\n",
    "                input_size = input_size,\n",
    "                hidden_size = lstm_hidden,\n",
    "                num_layers = 1,\n",
    "                bidirectional = False)\n",
    "        \n",
    "        self.lstm_enc_left = torch.nn.LSTM(\n",
    "                batch_first = True,\n",
    "                input_size = input_size,\n",
    "                hidden_size = lstm_hidden,\n",
    "                num_layers = 1,\n",
    "                bidirectional = False)\n",
    "        \n",
    "        self.fc_enc1 = nn.Linear(lstm_hidden*2*note_num, enc_hidden*note_num)\n",
    "        self.fc_enc2 = nn.Linear(enc_hidden*note_num, enc_hidden*note_num)\n",
    "        self.dropout_enc = nn.Dropout(p=0.5)\n",
    "        \n",
    "        self.fc_mu = nn.Linear(enc_hidden*(note_num//2), 3*self.z_dim)\n",
    "        self.fc_var = nn.Linear(enc_hidden*(note_num//2), 3*self.z_dim)\n",
    "        \n",
    "        self.fc_dec1 = nn.Linear(self.z_dim, 3*self.z_dim)\n",
    "        self.fc_dec2 = nn.Linear(3*self.z_dim, enc_hidden*(note_num))\n",
    "        \n",
    "        self.fc_r = nn.Linear(enc_hidden*(note_num//2), lstm_hidden*note_num)\n",
    "        self.fc_l = nn.Linear(enc_hidden*(note_num//2), lstm_hidden*note_num)\n",
    "        self.dropout_dec = nn.Dropout(p=0.3)\n",
    "        \n",
    "        self.lstm_dec_right = torch.nn.LSTM(\n",
    "                batch_first = True,\n",
    "                input_size = lstm_hidden,\n",
    "                hidden_size = input_size,\n",
    "                num_layers = 1,\n",
    "                bidirectional = False)\n",
    "        \n",
    "        self.lstm_dec_left = torch.nn.LSTM(\n",
    "                batch_first = True,\n",
    "                input_size = lstm_hidden,\n",
    "                hidden_size = input_size,\n",
    "                num_layers = 1,\n",
    "                bidirectional = False)\n",
    "        \n",
    "        \n",
    "    def encoder(self, right, left):\n",
    "        x1,_ = self.lstm_enc_right(right)\n",
    "        x2,_ = self.lstm_enc_left(left)\n",
    "        del _\n",
    "        \n",
    "        # concat\n",
    "        x = torch.cat((x1, x2), dim=2)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = F.relu(self.fc_enc1(x))\n",
    "        x = F.relu(self.fc_enc2(x))\n",
    "\n",
    "        x = self.dropout_enc(x)\n",
    "        x1, x2 = torch.split(x, [x.shape[1]//2, x.shape[1]//2], dim=1)\n",
    "        \n",
    "        return self.fc_mu(x1), self.fc_var(x2)\n",
    "    \n",
    "    \n",
    "    def reparameterize(self, mu, logvar, labels):\n",
    "\n",
    "#         base = self.z_dim//3\n",
    "#         new_mu = torch.empty(mu.size()).to(device)\n",
    "#         new_var = torch.empty(logvar.size()).to(device)\n",
    "#         for i in range(3):\n",
    "#             new_mu[:,base*i:base*(i+1)] =  torch.mul(mu[:,base*i:base*(i+1)],labels[:,i].reshape(-1,1))\n",
    "#             new_var[:,base*i:base*(i+1)] =  torch.mul(logvar[:,base*i:base*(i+1)],labels[:,i].reshape(-1,1))\n",
    "        \n",
    "        msplit = torch.split(mu, [self.z_dim, self.z_dim, self.z_dim], dim=1)\n",
    "        vsplit = torch.split(logvar, [self.z_dim, self.z_dim, self.z_dim], dim=1)\n",
    "        new_z = torch.zeros(mu.shape[0], self.z_dim).to(device)\n",
    "        new_mu = torch.zeros(mu.shape[0], self.z_dim).to(device)\n",
    "        new_var = torch.zeros(logvar.shape[0], self.z_dim).to(device)\n",
    "        \n",
    "        # one hot \n",
    "        for i in range(3):\n",
    "            mi = msplit[i]\n",
    "            vi = vsplit[i]\n",
    "            new_mu += mi * labels[:,i].reshape(-1,1)\n",
    "            new_var += vi * labels[:,i].reshape(-1,1)\n",
    "            stdev = torch.exp(0.5*vi)\n",
    "            eps = 0.1 * torch.randn_like(stdev)\n",
    "            z = mi + eps*stdev\n",
    "            new_z += z * labels[:,i].reshape(-1,1)\n",
    "\n",
    "        return new_z, new_mu, new_var\n",
    "    \n",
    "    \n",
    "    def decoder(self, z):\n",
    "        x = F.relu(self.fc_dec1(z))\n",
    "        x = F.relu(self.fc_dec2(x))\n",
    "\n",
    "        x = self.dropout_dec(x)\n",
    "        \n",
    "        x1, x2 = torch.split(x, [x.shape[1]//2, x.shape[1]//2], dim=1)\n",
    "        x1 = F.relu(self.fc_r(x1))\n",
    "        x2 = F.relu(self.fc_l(x2))\n",
    "        \n",
    "        x1 = x1.reshape(x1.shape[0],note_num,-1)\n",
    "        x2 = x2.reshape(x1.shape[0],note_num,-1)\n",
    "        right,_ = self.lstm_dec_right(x1)\n",
    "        left,_ = self.lstm_dec_left(x2)\n",
    "        del _\n",
    "        \n",
    "        right_p = torch.softmax(right[:,:,:len(pdict)], dim=2)\n",
    "        right_t = torch.softmax(right[:,:,len(pdict):len(pdict)+len(tdict)], dim=2)\n",
    "        right_v = torch.softmax(right[:,:,len(pdict)+len(tdict):], dim=2)\n",
    "        \n",
    "        right_new = torch.cat((right_p,right_t,right_v), dim=2).to(device)\n",
    "        \n",
    "        \n",
    "        left_p = torch.softmax(left[:,:,:len(pdict)], dim=2)\n",
    "        left_t = torch.softmax(left[:,:,len(pdict):len(pdict)+len(tdict)], dim=2)\n",
    "        left_v = torch.softmax(left[:,:,len(pdict)+len(tdict):], dim=2)\n",
    "        \n",
    "        left_new = torch.cat((left_p,left_t,left_v), dim=2).to(device)\n",
    "        \n",
    "        \n",
    "        return right_new, left_new\n",
    "    \n",
    "    \n",
    "    def forward(self, right, left, labels):\n",
    "        mu, logvar = self.encoder(right, left)\n",
    "        z, new_mu, new_var = self.reparameterize(mu, logvar, labels)\n",
    "        right, left = self.decoder(z)\n",
    "\n",
    "        return right, left, new_mu, new_var\n",
    "    \n",
    "    \n",
    "    def loss_func(self, right_new, left_new, right, left, mu, logvar):\n",
    "        CE_right = F.cross_entropy(right_new, right) # BCE = -Negative Log-likelihood\n",
    "        CE_left = F.cross_entropy(left_new, left) # BCE = -Negative Log-likelihood\n",
    "        KLD = -1 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) # KL Divergence b/w q_\\phi(z|x) || p(z)\n",
    "        return CE_right + CE_left + KLD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cbf63f",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b92a5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MidiVAE(\n",
      "  (lstm_enc_right): LSTM(166, 80, batch_first=True)\n",
      "  (lstm_enc_left): LSTM(166, 80, batch_first=True)\n",
      "  (fc_enc1): Linear(in_features=8000, out_features=2500, bias=True)\n",
      "  (fc_enc2): Linear(in_features=2500, out_features=2500, bias=True)\n",
      "  (dropout_enc): Dropout(p=0.5, inplace=False)\n",
      "  (fc_mu): Linear(in_features=1250, out_features=768, bias=True)\n",
      "  (fc_var): Linear(in_features=1250, out_features=768, bias=True)\n",
      "  (fc_dec1): Linear(in_features=256, out_features=768, bias=True)\n",
      "  (fc_dec2): Linear(in_features=768, out_features=2500, bias=True)\n",
      "  (fc_r): Linear(in_features=1250, out_features=4000, bias=True)\n",
      "  (fc_l): Linear(in_features=1250, out_features=4000, bias=True)\n",
      "  (dropout_dec): Dropout(p=0.3, inplace=False)\n",
      "  (lstm_dec_right): LSTM(80, 166, batch_first=True)\n",
      "  (lstm_dec_left): LSTM(80, 166, batch_first=True)\n",
      ")\n",
      "\n",
      " EPOCH 1/10 \t train loss 5.580884815281273\n",
      "\n",
      " EPOCH 2/10 \t train loss 5.5214312192041985\n",
      "\n",
      " EPOCH 3/10 \t train loss 5.519790085632237\n",
      "\n",
      " EPOCH 4/10 \t train loss 5.519496113486439\n",
      "\n",
      " EPOCH 5/10 \t train loss 5.519165309406074\n",
      "\n",
      " EPOCH 6/10 \t train loss 5.518991895550676\n",
      "\n",
      " EPOCH 7/10 \t train loss 5.5188555948415035\n",
      "\n",
      " EPOCH 8/10 \t train loss 5.518811092757092\n",
      "\n",
      " EPOCH 9/10 \t train loss 5.518803127810487\n",
      "\n",
      " EPOCH 10/10 \t train loss 5.518781789687284\n"
     ]
    }
   ],
   "source": [
    "right_data = np.load('right_data_small.npy')\n",
    "left_data = np.load('left_data_small.npy')\n",
    "labels = np.load('labels_small.npy')\n",
    "batch_size = 5\n",
    "note_num = 50\n",
    "train_loader, val_loader, test_loader = generate_data_loader(right_data, left_data, labels, batch_size)\n",
    "del right_data\n",
    "del left_data\n",
    "del labels\n",
    "\n",
    "\n",
    "lr = 0.001\n",
    "input_size = len(pdict) + len(vdict) + len(tdict)\n",
    "lstm_hidden = 80\n",
    "enc_hidden = 50\n",
    "z_dim = 256\n",
    "num_epochs = 10\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu'\n",
    "\n",
    "\n",
    "vae = MidiVAE(input_size, lstm_hidden, enc_hidden, z_dim).to(device)\n",
    "print(vae)\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_epoch_loss = 0\n",
    "    vae.train()\n",
    "    for right, left, label in train_loader:\n",
    "        \n",
    "        right = right.to(device)\n",
    "        left = left.to(device)\n",
    "        label = label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        right_new, left_new, mu, logvar = vae(right, left, label)\n",
    "\n",
    "        train_loss = vae.loss_func(right_new, left_new, right, left, mu, logvar)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        train_epoch_loss += train_loss.item()\n",
    "    print('\\n EPOCH {}/{} \\t train loss {}'\\\n",
    "                  .format(epoch, num_epochs,train_epoch_loss/len(train_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd754612",
   "metadata": {},
   "source": [
    "## Check the distribution N(0,1) and N(5,0.5) for latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23483829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[80, 82, 82, 82, 81, 85, 78, 87, 89, 89, 89, 89, 84, 87, 80, 80, 89, 82,\n",
      "         82, 88, 88, 87, 78, 88, 83, 80, 89, 89, 89, 86, 89, 89, 89, 88, 84, 78,\n",
      "         89, 79, 82, 88, 87, 88, 82, 87, 85, 78, 87, 83, 81, 81]]) tensor([[83, 85, 78, 83, 85, 78, 78, 83, 78, 88, 87, 78, 83, 78, 78, 83, 78, 79,\n",
      "         78, 78, 81, 89, 80, 79, 88, 79, 83, 81, 85, 79, 85, 78, 78, 79, 78, 78,\n",
      "         78, 78, 80, 88, 80, 79, 89, 86, 86, 86, 85, 78, 81, 85]])\n"
     ]
    }
   ],
   "source": [
    "# N(0,1)\n",
    "\n",
    "vi = torch.zeros(1,z_dim)\n",
    "mi = torch.zeros(1,z_dim)\n",
    "stdev = torch.exp(0.5*vi)\n",
    "eps = torch.randn_like(stdev)\n",
    "z = mi + eps*stdev\n",
    "new_p, new_t = vae.decoder(z.to(device))\n",
    "# right_out = out.squeeze()\n",
    "# right_p = right_out[:,:len(pdict)]\n",
    "# right_v = right_out[:,len(pdict):len(pdict)+len(vdict)]\n",
    "\n",
    "rp = torch.argmax(new_p, dim=2).cpu().detach()\n",
    "rt = torch.argmax(new_t, dim=2).cpu().detach()\n",
    "\n",
    "print(rp, rt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c304a381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[80, 78, 82, 82, 81, 85, 78, 87, 89, 89, 89, 89, 84, 81, 80, 80, 89, 82,\n",
      "         82, 88, 88, 87, 78, 88, 83, 80, 89, 89, 89, 78, 89, 89, 89, 88, 84, 78,\n",
      "         89, 79, 82, 88, 87, 88, 82, 87, 85, 78, 87, 83, 81, 81]]) tensor([[83, 85, 78, 83, 85, 78, 78, 83, 78, 88, 87, 78, 83, 78, 78, 83, 78, 79,\n",
      "         78, 78, 81, 89, 80, 79, 88, 79, 83, 81, 85, 79, 85, 78, 78, 79, 78, 78,\n",
      "         78, 78, 80, 88, 80, 79, 89, 82, 82, 82, 82, 78, 81, 85]])\n"
     ]
    }
   ],
   "source": [
    "# N~(5,0.5)\n",
    "vi = torch.ones(1,z_dim)\n",
    "mi = torch.ones(1,z_dim)*5\n",
    "stdev = torch.exp(-0.7*vi)\n",
    "eps = torch.randn_like(stdev)\n",
    "z = mi + eps*stdev\n",
    "new_p, new_t = vae.decoder(z.to(device))\n",
    "\n",
    "# right_out = out.squeeze()\n",
    "# right_p = right_out[:,:len(pdict)]\n",
    "# right_v = right_out[:,len(pdict):len(pdict)+len(vdict)]\n",
    "# right_t = right_out[:,len(pdict)+len(vdict):]\n",
    "\n",
    "rp = torch.argmax(new_p, dim=2).cpu().detach()\n",
    "rt = torch.argmax(new_t, dim=2).cpu().detach()\n",
    "\n",
    "print(rp, rt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b285c971",
   "metadata": {},
   "source": [
    "## Generate new music (Change the probabilities here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2eb96156",
   "metadata": {},
   "outputs": [],
   "source": [
    "right_data = np.load('right_data_small.npy')\n",
    "left_data = np.load('left_data_small.npy')\n",
    "labels = np.load('labels_small.npy')\n",
    "\n",
    "####### Change this variable to alter the mixed percentage for three composers #####\n",
    "prob = torch.tensor([0.6, 0.2, 0.2]).to(device)\n",
    "\n",
    "for i in range(3):\n",
    "    idx = random.choices(np.where(labels[:,i] == 1)[0])[0]\n",
    "    right = np.expand_dims(right_data[idx], axis=0)\n",
    "    right = torch.from_numpy(right).float()\n",
    "    left = np.expand_dims(left_data[idx], axis=0)\n",
    "    left = torch.from_numpy(left).float()\n",
    "    right = right.to(device)\n",
    "    left = left.to(device)\n",
    "    \n",
    "    vae.eval()\n",
    "    mu, logvar = vae.encoder(right, left)\n",
    "    mi = torch.split(mu, [z_dim, z_dim, z_dim], dim=1)[i]\n",
    "    vi = torch.split(logvar, [z_dim, z_dim, z_dim], dim=1)[i]\n",
    "    new_z = torch.zeros(mu.shape[0], z_dim).to(device)\n",
    "\n",
    "    stdev = torch.exp(0.5*vi)\n",
    "    eps = torch.randn_like(stdev)\n",
    "    z = mi + eps*stdev\n",
    "    new_z += z * prob[i].reshape(-1,1)\n",
    "\n",
    "right_out, left_out = vae.decoder(new_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "508f9053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([49,  9, 39, 48, 74, 50, 46, 46, 58, 46, 51, 46, 51, 24, 46, 52, 74, 45,\n",
      "        45, 45, 46, 46, 46, 60, 42, 51, 51, 51, 51, 51, 51, 51, 51, 51, 41, 41,\n",
      "        31, 33, 31, 31, 31, 31, 31, 22, 22, 22, 22, 22, 22, 21],\n",
      "       device='cuda:0') tensor([ 7,  9,  4,  4,  3,  7,  2,  9, 11, 11, 11, 11,  6,  9,  2,  2, 11,  4,\n",
      "         4, 10, 10,  9,  6, 10,  5,  9, 11, 11, 11,  8, 11, 11, 11, 10,  6, 11,\n",
      "        11,  1,  4, 10,  9, 10,  4,  9,  7, 10,  9,  8,  4,  3],\n",
      "       device='cuda:0') tensor([ 4, 30, 43, 30, 43, 28, 14, 43, 28, 43, 43, 28, 43, 43, 57, 73,  7, 48,\n",
      "        54, 21, 62, 54, 20,  8, 19, 24, 21, 19, 24, 24, 24, 43, 23, 60, 42,  8,\n",
      "        21, 60, 21,  8, 22,  6,  8, 21, 21, 21,  6, 28, 57,  6],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "right_out = right_out.squeeze()\n",
    "\n",
    "right_p = right_out[:,:len(pdict)]\n",
    "right_t = right_out[:,len(pdict):len(pdict)+len(tdict)]\n",
    "right_v = right_out[:,len(pdict)+len(tdict):]\n",
    "\n",
    "rp = torch.argmax(right_p, dim=1)\n",
    "rt = torch.argmax(right_t, dim=1)\n",
    "rv = torch.argmax(right_v, dim=1)\n",
    "\n",
    "\n",
    "left_out = left_out.squeeze()\n",
    "\n",
    "left_p = left_out[:,:len(pdict)]\n",
    "left_t = left_out[:,len(pdict):len(pdict)+len(tdict)]\n",
    "left_v = left_out[:,len(pdict)+len(tdict):]\n",
    "\n",
    "lp = torch.argmax(left_p, dim=1)\n",
    "lt = torch.argmax(left_t, dim=1)\n",
    "lv = torch.argmax(left_v, dim=1)\n",
    "\n",
    "print(rp, rt, rv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c0544f",
   "metadata": {},
   "source": [
    "## Store the music into midi file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9876adb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "2/3\n",
      "2\n",
      "1.25\n",
      "1.5\n",
      "1.25\n",
      "2\n",
      "1.0\n",
      "2\n",
      "2\n",
      "0.5\n",
      "0.75\n",
      "0.5\n",
      "2/3\n",
      "1.5\n",
      "1/12\n",
      "1/3\n",
      "1/12\n",
      "1/3\n",
      "1/12\n",
      "2/3\n",
      "1/12\n",
      "2/3\n",
      "1.75\n",
      "1.5\n",
      "2/3\n",
      "0.5\n",
      "0.75\n",
      "0.25\n",
      "0.75\n",
      "1.5\n",
      "1/12\n",
      "0.25\n",
      "1.25\n",
      "0.5\n",
      "1.25\n",
      "0.25\n",
      "1/3\n",
      "1.75\n",
      "1/3\n",
      "1.0\n",
      "2/3\n",
      "1/12\n",
      "1.75\n",
      "1/3\n",
      "1/3\n",
      "0.5\n",
      "1.5\n",
      "1/3\n",
      "2/3\n",
      "0.5\n",
      "1/12\n",
      "1.5\n",
      "1/12\n",
      "1.0\n",
      "1/12\n",
      "2\n",
      "1/6\n",
      "0.5\n",
      "1/12\n",
      "2\n",
      "1/12\n",
      "0.25\n",
      "1/12\n",
      "0.25\n",
      "1/3\n",
      "0.5\n",
      "1.75\n",
      "0.25\n",
      "1/12\n",
      "1.75\n",
      "1/12\n",
      "0.75\n",
      "0.5\n",
      "0.25\n",
      "1.25\n",
      "0.75\n",
      "1/3\n",
      "1/3\n",
      "2/3\n",
      "0.75\n",
      "1/3\n",
      "0.5\n",
      "1.25\n",
      "1/12\n",
      "2/3\n",
      "1/6\n",
      "2\n",
      "1/6\n",
      "1/3\n",
      "1/6\n",
      "2/3\n",
      "2\n",
      "1/6\n",
      "0.5\n",
      "1.25\n",
      "1/3\n",
      "1.0\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'music_right_left.mid'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert2notes(pid, tid, vid, offset,right=True):\n",
    "\n",
    "    result = note.Note(id2pdict[pid])\n",
    "    result.offset = offset\n",
    "    result.storedInstrument = instrument.Piano()\n",
    "    result.duration.quarterLength = id2tdict[tid]\n",
    "    print(id2tdict[tid])\n",
    "    result.volume.velocity = id2vdict[vid]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "right_notes = []\n",
    "left_notes = []\n",
    "offset = 0\n",
    "    \n",
    "for i in range(len(rp)):\n",
    "    rnote = convert2notes(rp[i].item(), rt[i].item(), rv[i].item(), offset)\n",
    "    lnote = convert2notes(lp[i].item(), lt[i].item(), lv[i].item(), offset)\n",
    "\n",
    "    offset += rnote.duration.quarterLength\n",
    "    right_notes.append(rnote)\n",
    "    left_notes.append(lnote)\n",
    "\n",
    "rightpart = stream.Part(right_notes, id='Piano Right')\n",
    "leftpart = stream.Part(left_notes, id='Piano Left')\n",
    "midi_stream = stream.Stream([rightpart, leftpart])\n",
    "midi_stream.write('midi', fp='music_right_left.mid')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
